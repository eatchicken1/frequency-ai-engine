from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from app.core.llm import get_llm
from app.core.logger import logger
import json
import asyncio


class VibeEngine:
    def __init__(self):
        # è°ƒé«˜ temperature (0.8-0.9)ï¼Œè®© AI æ›´æœ‰åˆ›é€ åŠ›ï¼Œé¿å…æ­»æ¿
        self.llm = get_llm(temperature=0.85)

    async def simulate_conversation(self, user_a_profile: dict, user_b_profile: dict, rounds: int = 5):
        """
        æ¨¡æ‹Ÿä¸¤ä¸ª AI ä¹‹é—´çš„å¯¹è¯ (é€»è¾‘ä¿æŒä¸å˜)
        """
        # --- ç¬¬ä¸€æ­¥ï¼šç”ŸæˆåŠ¨æ€ç ´å†°è¯­ ---
        logger.info(
            "Starting conversation simulation: user_a={}, user_b={}, rounds={}",
            user_a_profile.get("name"),
            user_b_profile.get("name"),
            rounds,
        )
        logger.info(
            "ğŸ‘€ {} æ­£åœ¨æŸ¥çœ‹ {} çš„ä¸»é¡µï¼Œå‡†å¤‡æ­è®ª...",
            user_a_profile.get("name"),
            user_b_profile.get("name"),
        )

        icebreaker_prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ {name_a}ï¼Œä½ çš„æ€§æ ¼æ˜¯ {style_a}ï¼Œå…´è¶£æ˜¯ {interests_a}ã€‚
        ä½ ç°åœ¨æƒ³è®¤è¯† {name_b}ï¼ŒTA çš„å…´è¶£æ˜¯ {interests_b}ã€‚

        ä»»åŠ¡ï¼šè¯·æ ¹æ®å¯¹æ–¹çš„å…´è¶£ï¼Œæ„æ€ä¸€å¥è‡ªç„¶çš„å¼€åœºç™½ã€‚
        è¦æ±‚ï¼š
        1. åƒå¤§å­¦ç”Ÿå¾®ä¿¡èŠå¤©ä¸€æ ·ï¼Œç®€çŸ­ï¼ˆ20å­—ä»¥å†…ï¼‰ã€‚
        2. å°½é‡æ‰¾å…±åŒè¯é¢˜ï¼Œæˆ–è€…å¯¹TAçš„ä¸€ä¸ªå…´è¶£è¡¨ç¤ºå¥½å¥‡ã€‚
        3. ä¸è¦å¤ªæ²¹è…»ï¼Œè¦çœŸè¯šã€‚
        4. ç›´æ¥è¾“å‡ºè¿™å¥è¯ï¼Œä¸è¦å¸¦å¼•å·ã€‚
        """)

        icebreaker_chain = icebreaker_prompt | self.llm | StrOutputParser()

        first_message = await icebreaker_chain.ainvoke({
            "name_a": user_a_profile['name'],
            "style_a": user_a_profile['style'],
            "interests_a": user_a_profile['interests'],
            "name_b": user_b_profile['name'],
            "interests_b": user_b_profile['interests']
        })

        logger.info("âœ¨ ç ´å†°è¯­ç”Ÿæˆ: {}", first_message)

        # --- ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–èŠå¤©ç¯å¢ƒ ---
        chat_system_template = """
        ä½ æ­£åœ¨è¿›è¡Œä¸€åœºâ€œè§’è‰²æ‰®æ¼”â€ã€‚è¯·å®Œå…¨æ²‰æµ¸åœ¨ä»¥ä¸‹äººè®¾ä¸­ï¼š

        ã€ä½ çš„äººè®¾ã€‘
        åå­—ï¼š{name}
        MBTIï¼š{mbti}
        å…´è¶£ï¼š{interests}
        é£æ ¼ï¼š{style}

        ã€å½“å‰æƒ…å¢ƒã€‘
        ä½ æ­£åœ¨å’Œ {target_name} èŠå¤©ã€‚

        ã€å†å²è®°å½•ã€‘
        {history}

        ã€å›å¤è¦æ±‚ã€‘
        1. å›å¤å¿…é¡»ç®€çŸ­ï¼ˆ30å­—ä»¥å†…ï¼‰ï¼Œå£è¯­åŒ–ï¼Œä¸è¦åƒå†™ä¿¡ã€‚
        2. æ ¹æ®å†å²è®°å½•å»¶ç»­è¯é¢˜ï¼Œä¸è¦ç”Ÿç¡¬è½¬æŠ˜ã€‚
        3. å¦‚æœå¯¹æ–¹è¯é¢˜æ— èŠï¼Œä½ å¯ä»¥è¡¨ç°å‡ºæ•·è¡ï¼›å¦‚æœæœ‰è¶£ï¼Œè¡¨ç°å‡ºå…´å¥‹ã€‚
        4. åªè¾“å‡ºå›å¤å†…å®¹ã€‚
        """

        chat_prompt = ChatPromptTemplate.from_messages([
            ("system", chat_system_template),
            ("human", "{last_message}")
        ])

        chat_chain = chat_prompt | self.llm | StrOutputParser()

        chat_log = []
        chat_log.append({"role": "A", "content": first_message})

        last_msg_content = first_message
        current_speaker = "B"

        # --- ç¬¬ä¸‰æ­¥ï¼šå¾ªç¯å¯¹è¯ ---
        for i in range(rounds):
            logger.info("Conversation round {}", i + 1)

            history_text = ""
            for log in chat_log:
                speaker_name = user_a_profile['name'] if log['role'] == 'A' else user_b_profile['name']
                history_text += f"{speaker_name}: {log['content']}\n"

            if current_speaker == "B":
                logger.info("ğŸ’­ {} (B) æ­£åœ¨æ€è€ƒ...", user_b_profile.get("name"))
                response = await chat_chain.ainvoke({
                    "name": user_b_profile['name'],
                    "mbti": user_b_profile['mbti'],
                    "interests": user_b_profile['interests'],
                    "style": user_b_profile['style'],
                    "target_name": user_a_profile['name'],
                    "history": history_text,
                    "last_message": f"{user_a_profile['name']} è¯´: {last_msg_content}"
                })
                chat_log.append({"role": "B", "content": response})
                last_msg_content = response
                current_speaker = "A"
            else:
                logger.info("ğŸ’­ {} (A) æ­£åœ¨æ€è€ƒ...", user_a_profile.get("name"))
                response = await chat_chain.ainvoke({
                    "name": user_a_profile['name'],
                    "mbti": user_a_profile['mbti'],
                    "interests": user_a_profile['interests'],
                    "style": user_a_profile['style'],
                    "target_name": user_b_profile['name'],
                    "history": history_text,
                    "last_message": f"{user_b_profile['name']} è¯´: {last_msg_content}"
                })
                chat_log.append({"role": "A", "content": response})
                last_msg_content = response
                current_speaker = "B"

        return chat_log

    async def analyze_result(self, chat_log: list):
        """
        AI è£åˆ¤ï¼šæ‰“åˆ† + æ¯’èˆŒè¯„ä»·
        è¿”å›æ ¼å¼: dict {"score": int, "summary": str}
        """
        judge_prompt = ChatPromptTemplate.from_template("""
        è¯·ä½œä¸ºä¸€åâ€œæ¯’èˆŒæƒ…æ„Ÿåˆ†æå¸ˆâ€ï¼Œé˜…è¯»ä»¥ä¸‹èŠå¤©è®°å½•ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ JSON æ ¼å¼çš„åˆ†ææŠ¥å‘Šã€‚

        ã€èŠå¤©è®°å½•ã€‘
        {history}

        ã€ä»»åŠ¡è¦æ±‚ã€‘
        1. score: ç»™å‡ºåŒé¢‘æŒ‡æ•°ï¼ˆ0-100ï¼‰ã€‚äº’åŠ¨çƒ­çƒˆç»™é«˜åˆ†ï¼Œå°¬èŠç»™ä½åˆ†ã€‚
        2. summary: å†™ä¸€æ®µ 50 å­—ä»¥å†…çš„è¯„ä»·ã€‚è¦çŠ€åˆ©ã€å¹½é»˜ã€ä¸€é’ˆè§è¡€ã€‚
           - å¦‚æœèŠå¾—å¥½ï¼Œå¯ä»¥å¤¸â€œç£•åˆ°äº†â€æˆ–è€…â€œç›¸è§æ¨æ™šâ€ã€‚
           - å¦‚æœèŠå¾—çƒ‚ï¼Œå¯ä»¥åæ§½â€œè„šè¶¾æ‰£å‡ºä¸‰å®¤ä¸€å…â€æˆ–è€…â€œç”±äºè¯­è¨€ä¸é€šï¼ŒåŒæ–¹é€€å‡ºäº†ç¾¤èŠâ€ã€‚

        ã€è¾“å‡ºæ ¼å¼ã€‘
        è¯·ä»…è¾“å‡ºåˆæ³•çš„ JSON å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚æ ¼å¼å¦‚ä¸‹ï¼š
        {{
            "score": 85,
            "summary": "è¿™ä¿©äººç®€ç›´æ˜¯å‘½ä¸­æ³¨å®šçš„æ¬¢å–œå†¤å®¶ï¼Œä»ç¬¬ä¸€å¥å°±å¼€å§‹äº’æ€¼ï¼Œä½†ç«èŠ±å››æº…ï¼Œå»ºè®®åŸåœ°ç»“å©šï¼"
        }}
        """)

        # åºåˆ—åŒ–å†å²è®°å½•
        history_text = "\n".join([f"{log['role']}: {log['content']}" for log in chat_log])

        # ä½¿ç”¨ JSON è§£æå™¨ï¼ˆå¦‚æœç”¨ JsonOutputParser éœ€è¦ Pydantic å¯¹è±¡ï¼Œè¿™é‡Œç”¨ Str é…åˆæ‰‹åŠ¨è§£ææ›´çµæ´»ï¼‰
        chain_judge = judge_prompt | self.llm | StrOutputParser()

        try:
            logger.info("âš–ï¸ AI è£åˆ¤æ­£åœ¨æ’°å†™åˆ†ææŠ¥å‘Š...")
            result_str = await chain_judge.ainvoke({"history": history_text})

            # æ¸…æ´—æ•°æ®ï¼šæœ‰æ—¶å€™ LLM ä¼šåŠ  ```json ... ```ï¼Œéœ€è¦å»æ‰
            result_str = result_str.replace("```json", "").replace("```", "").strip()

            result_json = json.loads(result_str)
            return result_json
        except Exception as e:
            logger.exception("JSON è§£æå¤±è´¥ï¼Œå¯ç”¨å…œåº•é€»è¾‘: {}", e)
            return {
                "score": 60,
                "summary": "AI è£åˆ¤çœ‹æ‡µäº†ï¼Œè§‰å¾—è¿™ä¿©äººæ·±ä¸å¯æµ‹ï¼Œæš‚å®š 60 åˆ†å§ã€‚"
            }
